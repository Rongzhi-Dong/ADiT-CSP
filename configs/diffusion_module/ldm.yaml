_target_: src.models.ldm_module.LatentDiffusionLitModule

# autoencoder_ckpt: "/home/rongzhi/ADiT-CSP/logs/train_autoencoder/runs/vae_latent@8_kl@0.00001_2026-01-19_23-43-03/checkpoints/vae-epoch@1399-step@148400-val_mp20_match_rate@0.8104.ckpt"  # path to VAE checkpoint

# autoencoder_ckpt: "/home/rongzhi/ADiT-CSP/logs/train_autoencoder/runs/vae_latent@8_kl@0.00001_2026-01-19_23-43-03/checkpoints/vae-epoch@2294-step@243270-val_mp20_match_rate@0.8103.ckpt"  # path to VAE checkpoint
autoencoder_ckpt: "/home/rongzhi/ADiT-CSP/logs/train_autoencoder/runs/vae_latent@8_kl@0.00001_2026-01-19_23-43-03/checkpoints/vae-epoch@3049-step@323300-val_mp20_match_rate@0.8126.ckpt"
# autoencoder_ckpt: "/home/rongzhi/ADiT-CSP/pre-ckpt/vae.ckpt"  ### pre-ckpt

denoiser:
  _target_: src.models.denoisers.dit.DiT
  d_x: 8
  d_model: 768    # 384, 768, 1024
  nhead: 12        # 6, 12, 16
  num_layers: 12  # 12, 12, 24
  num_datasets: 2 #2

interpolant:
  _target_: src.models.interpolants.flow_matching.FlowMatchingInterpolant
  min_t: 1e-2
  corrupt: True
  num_timesteps: 100
  self_condition: true
  self_condition_prob: 0.5

augmentations:
  frac_coords: true
  pos: true

sampling:
  cfg_scale: 2.0
  num_samples: 1000
  batch_size: 100
  data_dir: ${paths.data_dir}
  visualize: true
  save_dir: ${paths.viz_dir}
  removeHs: false  # only retain heavy atoms (for molecules)

conditioning:
  dataset_idx: true  # always true
  spacegroup: false

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.0001
  weight_decay: 0.0

scheduler: null

scheduler_frequency: ${trainer.check_val_every_n_epoch}

# compile model for faster training with pytorch 2.0
compile: false
